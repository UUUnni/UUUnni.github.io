

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>

<script src="https://cdn.jsdelivr.net/gh/Sanarous/files@1.151/js/clicklove.js"></script>



<head>
  <meta charset="UTF-8">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <!-- <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap" rel="stylesheet"> -->
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700;900&amp;display=swap" rel="stylesheet">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/sun.png">
  <link rel="icon" href="/img/sun.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#252d38">
  <meta name="author" content="31">
  <meta name="keywords" content="">
  
    <meta name="description" content="针对智能家居中智能音箱的语音识别，为了区分人声和录音，对于在不同的环境、位置对于人声的识别挑战，提出了array fingerprint的人声特征，进而提出了ARRAYID轻量级被动检测方案">
<meta property="og:type" content="article">
<meta property="og:title" content="Your Microphone Array Retains Your Identity">
<meta property="og:url" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/index.html">
<meta property="og:site_name" content="31&#39;s Blog">
<meta property="og:description" content="针对智能家居中智能音箱的语音识别，为了区分人声和录音，对于在不同的环境、位置对于人声的识别挑战，提出了array fingerprint的人声特征，进而提出了ARRAYID轻量级被动检测方案">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182448675.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182534270.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182657141.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182805903.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182834688.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183106103.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183135813.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183158418.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183213384.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183221836.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183243135.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183420335.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183513213.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183912066.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184035840.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184133622.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184206444.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184224827.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184240133.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184255932.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184304977.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184315506.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184327622.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184401256.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184411640.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184420107.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184428463.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184440250.png">
<meta property="og:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184450864.png">
<meta property="article:published_time" content="2022-12-14T10:12:37.000Z">
<meta property="article:modified_time" content="2022-12-14T11:05:49.879Z">
<meta property="article:author" content="31">
<meta property="article:tag" content="Audio">
<meta property="article:tag" content="CyberSecurity">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182448675.png">
  
  
  <title>Your Microphone Array Retains Your Identity - 31&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/night-owl.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/mac.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"uuunni.github.io","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":50,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"always","icon":"#"},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":6},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"boxDRW3XpqvL2o48tPQxzy8N-gzGzoHsz","app_key":"t8snqjGhwT0Sery7lIU4V169","server_url":"https://boxdrw3x.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>31</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/3.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Your Microphone Array Retains Your Identity">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-12-14 18:12" pubdate>
        2022年12月14日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      9.6k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      81 分钟
    </span>
  

  
  
    
      <!-- LeanCloud 统计文章PV -->
      <span id="leancloud-page-views-container" class="post-meta" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="leancloud-page-views"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Your Microphone Array Retains Your Identity</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：2022年12月14日 晚上
                
              </p>
            
            <div class="markdown-body">
              <blockquote>
<p>[Title]</p>
<p>Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers</p>
<p>[Conference]</p>
<p>31st USENIX Security Symposium (USENIX Security 22)</p>
<p><a target="_blank" rel="noopener" href="https://www.usenix.org/conference/usenixsecurity22/presentation/meng">URL</a></p>
</blockquote>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>智能音箱已经是智能家居的控制枢纽，所以攻击它，很危险：</p>
<ul>
<li>replay attack</li>
<li><p>flaws in smart speaker</p>
<ul>
<li>inaudible ultrasound-based attacks</li>
<li>deep learning models</li>
</ul>
</li>
</ul>
<p>研究者将这些攻击归结于真人和电子设备发出声音的区别，把这个特征称为 liveness。现有的liveness-detection 分为两种:</p>
<ul>
<li><p>multi-factor authentication</p>
<p>利用音频和其他物理量（加速度、电磁场、超声波、WIFI、mm-Wave）区分人声和机器合成声</p>
</li>
<li><p>passive scheme</p>
<p>只利用音频。因为真人与机器发音方式的差异会导致音频频谱图中细微但显著的差异</p>
</li>
</ul>
<p>仍面临的挑战 usability efficiency：</p>
<ul>
<li>为了获得物理量信息，用户需要佩戴传感器（加速度、电磁场）或者主动发射信号（超声波、WIFI）</li>
<li>只利用音频的话容易受到声音传播信道变化的影响和基于频谱调制的攻击</li>
<li>为了保证鲁棒性需要用户保持固定的姿势</li>
</ul>
<p>所以，一个好的liveness detection应该具备以下条件：</p>
<ul>
<li>Device-free</li>
<li>Resilient to environment change</li>
<li>High accuracy</li>
</ul>
<p>Motivation: 主流智能音箱的麦克风阵列，由于麦克风的位置和相互距离不同，将显著增强收集音频的多样性，进而实现 device-free, robustness, accuracy</p>
<p>Key challenges:</p>
<ul>
<li>1: What is the advantage of adopting a microphone array compared with a single microphone?</li>
<li>2: How can we eliminate the distortions caused by environment factors by leveraging the microphone array?</li>
<li>3: How can we demonstrate the effectiveness and accuracy of the proposed scheme?</li>
</ul>
<h1 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h1><h2 id="2-1-Threat-Model"><a href="#2-1-Threat-Model" class="headerlink" title="2.1 Threat Model"></a>2.1 Threat Model</h2><p>voice spoofing attack 语音欺骗攻击可分为两类：</p>
<ul>
<li><p>Classical replay attacks</p>
<p>是本文主要研究的</p>
</li>
<li><p>Advanced adversarial attacks</p>
<p>voice synthesized technique</p>
<p>modify the spectrum of audio</p>
</li>
</ul>
<h2 id="2-2-Sound-Generation-and-Propagation"><a href="#2-2-Sound-Generation-and-Propagation" class="headerlink" title="2.2 Sound Generation and Propagation"></a>2.2 Sound Generation and Propagation</h2><p>声音的产生和传播过程</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182448675.png" srcset="/img/loading.gif" lazyload alt="Sound Generation and Propagation"></p>
<p>signal: $x(f,t)$</p>
<p>sound wave: $s(f,t)=h_{dev}(f,t)\cdot x(f,t)$</p>
<p>air pressure: $y(f,t)=h_{air}(d,f,t)\cdot s(f,t)$</p>
<p>$f$ —— 频率</p>
<p>$t$ —— 时间</p>
<p>$h_{dev}()$ —— 调制声音信号中的信道增益</p>
<p>$d$ —— 麦克风和声源的距离</p>
<p>$h_{air}()$ —— 声音信号在空气传播中的信道增益</p>
<h2 id="2-3-Passive-Liveness-Detection"><a href="#2-3-Passive-Liveness-Detection" class="headerlink" title="2.3 Passive Liveness Detection"></a>2.3 Passive Liveness Detection</h2><h3 id="2-3-1-Mono-Channel-based-Detection"><a href="#2-3-1-Mono-Channel-based-Detection" class="headerlink" title="2.3.1 Mono Channel-based Detection"></a>2.3.1 Mono Channel-based Detection</h3><p>忽略声音传播过程中的失真，$h_{air}(d,f,t)$ 可以被看作一个常量$A$</p>
<script type="math/tex; mode=display">
y_{auth}(d,f,t)=A\cdot h_{user}(f,t)\cdot x(f,t)
\\
y_{spoof}(d,f,t)=A\cdot h_{dev}(f,t)\cdot x(f,t)</script><p>约掉相同的项，接收到的音频已经包含了声源的身份信息</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182534270.png" srcset="/img/loading.gif" lazyload alt></p>
<p>两个音频样本的sub-bass区(20-300Hz)有很大不同</p>
<p>但是，$h_{air}(d,f,t)$在不同环境下会受影响（周围物体的形状、材质，声音传播路径，空气吸收指数），不能一直被视作一个常量</p>
<h3 id="2-3-2-Fieldprint-based-Detection"><a href="#2-3-2-Fieldprint-based-Detection" class="headerlink" title="2.3.2 Fieldprint-based Detection"></a>2.3.2 Fieldprint-based Detection</h3><p>不同声源会在其周围形成一个独特的声场，通过测量这个场的特性，可以推断出声源身份</p>
<script type="math/tex; mode=display">
Field=\log{(\frac{y_1(f,t)}{y_2(f,t)})}</script><p>但是，如果要获得稳定准确的声场信息，需要声源和传感器相对稳定</p>
<h1 id="3-Array-Fingerprint"><a href="#3-Array-Fingerprint" class="headerlink" title="3 Array Fingerprint"></a>3 Array Fingerprint</h1><p>RQ1: How can we model the sound propagation in smart speaker scenarios and answer why existing features (e.g., fieldprint) cannot be effective in such scenarios?</p>
<p>RQ2: How can we extract a useful feature from multichannel voice samples that is robust regarding a user’s location and microphone array’s layout?</p>
<p>RQ3: What are the benefits of the array fingerprint? Is it effective and robust to the distortions caused by environmental factors?</p>
<h2 id="3-1-Theoretical-Analysis-on-Sound-Propagation-for-Smart-Speakers"><a href="#3-1-Theoretical-Analysis-on-Sound-Propagation-for-Smart-Speakers" class="headerlink" title="3.1 Theoretical Analysis on Sound Propagation for Smart Speakers"></a>3.1 Theoretical Analysis on Sound Propagation for Smart Speakers</h2><p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182657141.png" srcset="/img/loading.gif" lazyload alt></p>
<p>声源$S_0(L,0)$</p>
<p>麦克风阵列圆形分布$M_k$</p>
<p>应用经典的球形声波传播模型</p>
<script type="math/tex; mode=display">
h_{air}(d_k,f,t)=Ce^{-\alpha_cd_k}=Ce^{-\alpha(s(f,t))d_k}</script><p>那么第k个麦克风处的收集到的音频信息为</p>
<script type="math/tex; mode=display">
y_k(f,t)=h_{air}(d_k,f,t)\cdot s(f,t)=Ce^{-\alpha(s(f,t))d_k}\cdot s(f,t)</script><p><strong>mono channel-based detection的缺陷</strong></p>
<p>观察该式，发现声源与麦克风的距离的变化对结果的影响是非线性的，所以位置的变化带来的失真很难被消除</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182805903.png" srcset="/img/loading.gif" lazyload alt></p>
<p><strong>figerprint-based detection的缺陷</strong></p>
<p>代入先前的式子得</p>
<script type="math/tex; mode=display">
\log(y_i/y_j)=-\alpha(s(f,t))\cdot \lg(e)\cdot(d_i-d_j)</script><p>当麦克风的相对位置固定时，$d_i-d_j$ 可以看作是常量；当不固定时，使用这种方案变得不可行</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214182834688.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="3-2-Advantage-of-Array-Fingerprint-Definition-and-Simulation-based-Demonstration"><a href="#3-2-Advantage-of-Array-Fingerprint-Definition-and-Simulation-based-Demonstration" class="headerlink" title="3.2 Advantage of Array Fingerprint: Definition and Simulation-based Demonstration"></a>3.2 Advantage of Array Fingerprint: Definition and Simulation-based Demonstration</h2><p>array fingerprint的定义如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
A_F&=std(\log [y_1,y_2,…,y_N])
\\&=std(C-\alpha(s(f,t))\cdot \lg(e)\cdot[d_1,d_2,...,d_N])
\\&=-\alpha(s(f,t))\cdot\lg(e)\cdot std([d_1,d_2,...d_k])
\\&=A_F(s(f,t),\sigma_d)
\end{aligned}</script><p>最终array fingerprint只与声源音频和传播距离的标准差相关</p>
<p>为了从$s(f,t)$ 获得声源的身份信息，$\sigma_d$ 必须是常量，下面证明它</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183106103.png" srcset="/img/loading.gif" lazyload alt></p>
<script type="math/tex; mode=display">
d_k=|\vec{M_kS_0}|=r\sqrt{1+(\frac{l}{r})^2-2(\frac{l}{r})\cos(\theta+\frac{2\pi(k-1)}{N})}</script><p>然后使用 $r=5cm$ 的Amazon Echo 3rd Gen音箱进行模拟</p>
<p> $L\in[1m,3m], \theta\in[0,90], N\in\{2,4,6,8\}$ 时的$\sigma_d$</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183135813.png" srcset="/img/loading.gif" lazyload alt></p>
<p>观察上图，当N超过4个之后，$\sigma_d$ 会趋向一个常量</p>
<p>而实际中智能音箱的麦克风通常多于4个，所以$\sigma_d$ 是几乎不变的，可以将它视作一个常量，几乎不影响array fingerprint</p>
<p>综上，array fingerprint只与声源音频$s(f,t)$ 有关，而且对环境因素的变化适应性强</p>
<h2 id="3-3-Validation-of-Array-Fingerprint"><a href="#3-3-Validation-of-Array-Fingerprint" class="headerlink" title="3.3 Validation of Array Fingerprint"></a>3.3 Validation of Array Fingerprint</h2><p>通过一系列的真实案例研究证明array fingerprint的有效性</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183158418.png" srcset="/img/loading.gif" lazyload alt></p>
<p>show the distinctiveness</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183213384.png" srcset="/img/loading.gif" lazyload alt></p>
<h1 id="4-The-Design-of-ARRAYID"><a href="#4-The-Design-of-ARRAYID" class="headerlink" title="4 The Design of ARRAYID"></a>4 The Design of ARRAYID</h1><p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183221836.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="4-1-Multi-channel-Data-Collection"><a href="#4-1-Multi-channel-Data-Collection" class="headerlink" title="4.1 Multi-channel Data Collection"></a>4.1 Multi-channel Data Collection</h2><p>使用开放的带有语音接口的模块化开发板收集数据</p>
<p>send $V$ to the next module</p>
<h2 id="4-2-Data-Pre-processing"><a href="#4-2-Data-Pre-processing" class="headerlink" title="4.2 Data Pre-processing"></a>4.2 Data Pre-processing</h2><p><strong>频率分析$s(f,t)$</strong></p>
<p>STFT 短时傅里叶变换+FFT快速傅里叶变换</p>
<blockquote>
<p><a href="[https://www.sohu.com/a/114138508_468636](https://www.sohu.com/a/114138508_468636">傅里叶变换</a>)</p>
<p>傅里叶变换FT不适合处理非平稳信号</p>
<p>所以——加窗，把整个时域过程分解成无数个等长的小过程，每个小过程近似平稳，再傅里叶变换，就知道在哪个时间点上出现的频率了——短时傅里叶变换</p>
</blockquote>
<p><strong>方向检测</strong></p>
<p>找到离声源最近的麦克风</p>
<ol>
<li><p>100Hz的高通滤波器</p>
</li>
<li><p>找alignment error最小的</p>
<p>$E_i=mean((V(:,i-1)-V(:,i))^2)$</p>
</li>
</ol>
<h2 id="4-3-Feature-Extraction"><a href="#4-3-Feature-Extraction" class="headerlink" title="4.3 Feature Extraction"></a>4.3 Feature Extraction</h2><h3 id="4-3-1-Spectrogram-Array-Feature"><a href="#4-3-1-Spectrogram-Array-Feature" class="headerlink" title="4.3.1 Spectrogram Array Feature"></a>4.3.1 Spectrogram Array Feature</h3><p>只保留低于$f_{sap}=5kHz$ 的频率分量</p>
<p>采样率$F_s=48kHz$</p>
<p>FFT点 $N_{fft}=4096$</p>
<p>那么保留下来的频谱图$Spec_k=S_k(:M_{spec},:)$</p>
<p>$M_{spec}=[\frac{f_{sap}\times N_{fft}}{F_s}]=426$</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183243135.png" srcset="/img/loading.gif" lazyload alt></p>
<p>然后将$Spec_k$ 转为网格矩阵$G_k$</p>
<script type="math/tex; mode=display">
G_k(i,j)=sum(Spec_k(1+(i-1)\cdot S_M:i\cdot S_M,1+(j-1)\cdot S_N:j\cdot S_N))
\\
S_M=[\frac{M_{spec}}{M_G}], S_N=[\frac{N_{spec}}{N_G}]</script><p>就是求每一块网格区域的总和</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183420335.png" srcset="/img/loading.gif" lazyload alt="image-20221214183420335"></p>
<p>然后计算array fingerprint</p>
<script type="math/tex; mode=display">
F_G(i,j)=std([G_1(i,j),G_2(i,j),…,G_N(i,j)])</script><p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183513213.png" srcset="/img/loading.gif" lazyload alt></p>
<p>但是观察到$F_G(:,i)$ 不同，是因为人本身讲不同音节时的发声部位不同</p>
<p>为了获得持续时间中某人声的通性，在时间域上取均值$\overline{F_G}$，5-point moving average and normalization 得到 array fingerprint $F_{SAP}$</p>
<p>一个有效性测试。因为需要快速实时响应，所以array fingerprint必须轻量化，所以将 $F_{SAP}$ 重采样为$N_{SAP}$ points</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214183912066.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="4-3-2-Spectrogram-Distribution-Feature"><a href="#4-3-2-Spectrogram-Distribution-Feature" class="headerlink" title="4.3.2 Spectrogram Distribution Feature"></a>4.3.2 Spectrogram Distribution Feature</h3><p>频谱图的分布特征 $F_{SDP}$</p>
<script type="math/tex; mode=display">
Ch_k(i)=\sum_{j=1}^{M_{spec}}S_k(j,i)</script><p>取均值并重采样到长度$N_{Ch}$</p>
<script type="math/tex; mode=display">
\overline{Ch(i)}=mean([CH_1(i),Ch_2(i),…,Ch_N(i)])</script><p>然后获得$CH_k$ 的分布，计算累积分布函数$Cum_k$</p>
<script type="math/tex; mode=display">
Cum_k(\mu(k,i))\le Thr_i\le Cum_k(\mu(k,i)+1)
\\
Thr=[0.1,0.3,0.5,0.7,0.9]</script><p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184035840.png" srcset="/img/loading.gif" lazyload alt></p>
<p>可以获得k组$N\times 5$指数$\mu$ ，再算均值和标准差</p>
<script type="math/tex; mode=display">
D_{mean}(i)=mean(\mu(:,i))
\\
D_{std}(i)=std(\mu(:,i))</script><p>最终$F_{SDP}=[\overline{Ch},D_{mean},D_{std}]$</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184133622.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="4-3-3-Channel-LPCC-Features"><a href="#4-3-3-Channel-LPCC-Features" class="headerlink" title="4.3.3 Channel LPCC Features"></a>4.3.3 Channel LPCC Features</h3><p>Linear Prediction Cepstrum Coefficients 线性预测编码倒谱系数</p>
<p>每个声道有自己的物理特性</p>
<p>$F_{LPC}$ 只保留最近的麦克风和它反面的麦克风的</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184206444.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="4-4-Classification-Model"><a href="#4-4-Classification-Model" class="headerlink" title="4.4 Classification Model"></a>4.4 Classification Model</h2><p>特征向量$X=[F_{SAP},F_{SDP},F_{LPC}]$</p>
<p>lightweight feed-forward back-propagation neural network</p>
<p>3 hidden layers (64, 32, 16) with relu activation5</p>
<h1 id="5-Evaluations"><a href="#5-Evaluations" class="headerlink" title="5 Evaluations"></a>5 Evaluations</h1><h2 id="5-1-Experiment-Setup"><a href="#5-1-Experiment-Setup" class="headerlink" title="5.1 Experiment Setup"></a>5.1 Experiment Setup</h2><p><strong>Hardware setup</strong></p>
<p>2 open modular development boards to collect multi-channel audios</p>
<ul>
<li>Matrix Creator: 8 microphones, r=5.4cm</li>
<li>Seeed ReSpeaker Core v2: 6 microphones, r=4.7cm</li>
</ul>
<p>14 different electrical devices as spoofing devices</p>
<p><strong>Data collection procedure</strong></p>
<p>20 participants</p>
<p>20 different voice commands, 4 distances</p>
<ul>
<li>Authentic audio collection</li>
<li>Spoofing audio collection</li>
</ul>
<p><strong>Dataset description</strong></p>
<p>MALD dataset: 32780 audio samples</p>
<p><strong>Training procedure</strong></p>
<p>train ARRAYID: two-fold cross-validation (the training and testing datasets are divided equally)</p>
<p>50% for generating a classifier</p>
<p>30% for validation</p>
<p><strong>Evaluation metrics</strong></p>
<ul>
<li><p>accuracy</p>
<p>the percentage of the correctly recognized samples among all samples</p>
</li>
<li><p>false acceptance rate (FAR)</p>
<p>the rate at which a spoofing sample is wrongly accepted by ARRAYID</p>
</li>
<li><p>false rejection rate (FRR)</p>
<p>the rate at which an authentic sample is falsely rejected</p>
</li>
<li><p>true rejection rate (TRR)</p>
</li>
<li><p>equal error rate (EER)</p>
<p>the rate at which FAR = FRR</p>
</li>
</ul>
<h2 id="5-2-Perfomance-of-ARRAYID"><a href="#5-2-Perfomance-of-ARRAYID" class="headerlink" title="5.2 Perfomance of ARRAYID"></a>5.2 Perfomance of ARRAYID</h2><p><strong>Overall accuracy</strong></p>
<p>accuracy = 99.84%</p>
<p>FAR = 13/22539 = 0.05%</p>
<p>FRR = 40/10241 = 0.39%</p>
<p>ERR = 0.17%</p>
<p><strong>Per-user breakdown analysis</strong></p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184224827.png" srcset="/img/loading.gif" lazyload alt></p>
<p>False Acceptance cases only exist in 6 users</p>
<p>False Rejection cases are distributed among 14 users</p>
<p>But even for the worst-case, the detection accuracy is still at 99.0%</p>
<p><strong>Time overhead</strong></p>
<p>Intel i7-7700T CPU, 16 GB RAM</p>
<p>6-channel audios: 0.12s</p>
<p>8-channel audios: 0.38s</p>
<p><strong>Comparison with previous works</strong></p>
<p>To eliminate the potential bias in MALD dataset</p>
<p>ReMasc Core dataset: 12023 voice samples, 40 users, indoor/outdoor/vehicle environments</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184240133.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="5-3-Impact-of-Various-Factors-on-ARRAYID"><a href="#5-3-Impact-of-Various-Factors-on-ARRAYID" class="headerlink" title="5.3 Impact of Various Factors on ARRAYID"></a>5.3 Impact of Various Factors on ARRAYID</h2><p><strong>Impact of changing distance</strong></p>
<p>3 participants, 3 locations (1.2m, 1.8m, 2.4m)</p>
<p>train test 使用 different distances</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184255932.png" srcset="/img/loading.gif" lazyload alt></p>
<p><strong>Impact of changing direction</strong></p>
<p>10 participants, 4 directions (front, left, right, back)</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184304977.png" srcset="/img/loading.gif" lazyload alt></p>
<p><strong>Impact of user movement</strong></p>
<p>speak/hold a spoofing device while walking</p>
<p>detection accuracy = 98.2%</p>
<p><strong>Impact of changing environment</strong></p>
<p>launch voice spoofing at a different room</p>
<p>detection accuracy = 99.30%</p>
<p><strong>Impact of microphone numbers in the smart speaker</strong></p>
<p>从Matrix的8-channel数据抽出$(M_1,M_3,M_5,M_7)$ 新生成4-channel audio data</p>
<p>after conducting two-fold cross-validation on each group, the detection accuracies are:</p>
<p>4-channel = 99.78%</p>
<p>6-channel = 99.82%</p>
<p>8-channel = 99.90%</p>
<p>所以4以上的声道数量的变化影响不会很大，如同先前理论分析的一样</p>
<p>只要麦克风阵列呈圆形分布，ARRAYID就可以提供robust protection on thwarting voice spoofing</p>
<p><strong>Impact of Spoofing Devices</strong></p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184315506.png" srcset="/img/loading.gif" lazyload alt></p>
<p>(👆 to reduce the user’s enrollment burden, set the training proportion as 10%)</p>
<p>100% detection accuracy on 5 devices</p>
<p>even in the worst case, the true rejection rate is still at 95.86%</p>
<h2 id="5-4-Robustness-of-ARRAYID"><a href="#5-4-Robustness-of-ARRAYID" class="headerlink" title="5.4 Robustness of ARRAYID"></a>5.4 Robustness of ARRAYID</h2><h3 id="5-4-1-Handling-the-Incomplete-Enrollment-Procedure"><a href="#5-4-1-Handling-the-Incomplete-Enrollment-Procedure" class="headerlink" title="5.4.1 Handling the Incomplete Enrollment Procedure"></a>5.4.1 Handling the Incomplete Enrollment Procedure</h3><p>参与者需要参与authentic and spoofing 两个音频样本的收集。考虑到参与者没有完整参与的情况:</p>
<p><strong>Case 1: handling users who did not participate in any enrollment procedure</strong></p>
<p>对于MALD数据集中的每个用户，使用其他19个用户的所有样本来训练分类器，将该用户的样本作为测试集，结果精度及对比如下：</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184327622.png" srcset="/img/loading.gif" lazyload alt></p>
<p>overall accuracy: 99.84% to 92.97%</p>
<p>the worst case: 99.87% to 74.53%</p>
<p>for 11 users: still higher than 95%</p>
<p>证明ARRAYID处理未知用户的能力不稳定</p>
<p>To be improved!</p>
<p><strong>Case 2: handling a user with only authentic samples</strong></p>
<p>选取MALD数据集中#1~#18，对每个用户，使用该用户的authentic和其他17个用户的spoofing来训练，使用该用户的攻击样本进行测试，通过计算相关指标（如TRR）来评估</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184401256.png" srcset="/img/loading.gif" lazyload alt></p>
<p>overall accuracy: 99.96% to 99.68%</p>
<p>for 11 users: remains 100%</p>
<p>for 7 users: above 96%</p>
<p>Effective!</p>
<h3 id="5-4-2-Liveness-Detection-on-Noisy-Environments"><a href="#5-4-2-Liveness-Detection-on-Noisy-Environments" class="headerlink" title="5.4.2 Liveness Detection on Noisy Environments"></a>5.4.2 Liveness Detection on Noisy Environments</h3><p>the classifier where the noise level is 30 dB</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184411640.png" srcset="/img/loading.gif" lazyload alt></p>
<p>noise level from 45 dB to 65 dB: accuracy from 98.8% to 86.3%</p>
<p>work well when background noise &lt; 50 dB</p>
<p>To be improved!</p>
<h3 id="5-4-3-Defending-against-Advanced-Spoofing-Attacks"><a href="#5-4-3-Defending-against-Advanced-Spoofing-Attacks" class="headerlink" title="5.4.3 Defending against Advanced Spoofing Attacks"></a>5.4.3 Defending against Advanced Spoofing Attacks</h3><p><strong>Thwarting modulated attacks</strong></p>
<p>调制replayed audio的频谱：首先要获得目标用户的authentic voice samples；然后测量spoofing device的frequency amplitude curve和相关inverse filter；最后对authentic audio应用inverse filter并且通过spoofing device重放，收集到的频谱就会和authentic audio频谱相近。</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184420107.png" srcset="/img/loading.gif" lazyload alt></p>
<p>这种方法可以绕过许多现有的liveness detection，但是array fingerprints仍能显示authentic与modulated的不同。因为人和设备不能被视作点声源，我们的方案使用了多个麦克风收集音频</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184428463.png" srcset="/img/loading.gif" lazyload alt></p>
<p>并且通过实验，生成了三个设备的modulated attack samples并应用分类器，准确率为100%, 92.74%, 97.29%</p>
<p>发现在不同设备上的表现有差距</p>
<p>To be improved! - combine ARRAYID with the dual-domain detection</p>
<p><strong>Other adversarial example attacks</strong></p>
<p>hidden voice attack / VMask</p>
<p>detects 100%</p>
<p>reason: these attacks only aim to add subtle noises into source audio</p>
<h1 id="6-Discussions"><a href="#6-Discussions" class="headerlink" title="6 Discussions"></a>6 Discussions</h1><h2 id="6-1-User-Enrollment-Time-in-Training"><a href="#6-1-User-Enrollment-Time-in-Training" class="headerlink" title="6.1 User Enrollment Time in Training"></a>6.1 User Enrollment Time in Training</h2><p><strong>Impact of training dataset size</strong></p>
<p>To reduce the user’s registration burden</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184440250.png" srcset="/img/loading.gif" lazyload alt></p>
<p><strong>Time overhead of user’s enrollment</strong></p>
<p>10% training proportion</p>
<p>each user provides 51 authentic samples</p>
<p>average time length of voice command &lt; 3s</p>
<p>the enrollment can be done &lt; 3m</p>
<p>is ACCEPTABLE in real-world scenarios</p>
<h2 id="6-2-Distinguish-between-Different-Users"><a href="#6-2-Distinguish-between-Different-Users" class="headerlink" title="6.2 Distinguish between Different Users"></a>6.2 Distinguish between Different Users</h2><p>select 250 authentic samples from 5 users</p>
<p>utilize t-SNE to reduce the dimension, the feature vectors from different users are visually clustered</p>
<p><img src="/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/image-20221214184450864.png" srcset="/img/loading.gif" lazyload alt></p>
<p>经实验，speaker recognition accuracy = 99.88%</p>
<h2 id="6-3-Limitations-and-Countermeasures"><a href="#6-3-Limitations-and-Countermeasures" class="headerlink" title="6.3 Limitations and Countermeasures"></a>6.3 Limitations and Countermeasures</h2><p><strong>The user’s burden on the enrollment</strong></p>
<p>incorporate the enrollment into daily use</p>
<p>divide ARRAYID into:</p>
<ul>
<li><p>working phase</p>
<p>collect the audio and save the extracted features</p>
</li>
<li><p>idle phase</p>
<p>automatically update the classifier</p>
</li>
</ul>
<p>自动重训练可能会隐含其他风险</p>
<p><strong>Impact of noise and other speakers</strong></p>
<p>the strong noise or other speaker’s voice existing in the collected audios will inevitably degrade its performance.</p>
<p><strong>Temporal stability of array fingerprint</strong></p>
<p>the generated feature may be variant when the participant changes her/his speaking manner or suffers from mood swings</p>
<p>a feasible solution: daily use</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/paper/">paper</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Audio/">Audio</a>
                    
                      <a class="hover-with-bg" href="/tags/CyberSecurity/">CyberSecurity</a>
                    
                  </div>
                
              </div>
              <!-- 
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
               -->
              
                <p class="note note-info" style="background: #2e3a49; color: #eee;">
                <strong>本文作者: </strong><a href="/">31</a> <br>
                <strong>本文链接: </strong><a href="http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/">http://uuunni.github.io/2022/12/14/Your-Microphone-Array-Retains-Your-Identity/</a> <br>
                <strong>版权声明: </strong>本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                </p>
              

              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/10/14/CS161/">
                        <span class="hidden-mobile">CS161 Computer Security</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"QJtn4PrebEYsIJPwEkfclcO0-gzGzoHsz","appKey":"kdxH4zfnh96BgUrFbIGbhUVE","path":"window.location.pathname","placeholder":"留下点什么叭... ᶘ ᵒᴥᵒᶅ","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          Fluid.plugins.initFancyBox('#valine .vcontent img:not(.vemoji)');
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  

  
</footer>

<div class="text-center">
  <span id="timeDate">载入天数...</span>
  <span id="times">载入时分秒...</span>
  <script>
  var now = new Date();
  function createtime(){
      var grt= new Date("01/15/2022 00:00:00");//此处修改你的建站时间或者网站上线时间
      now.setTime(now.getTime()+250);
      days = (now - grt ) / 1000 / 60 / 60 / 24;
      dnum = Math.floor(days);
      hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum);
      hnum = Math.floor(hours);
      if(String(hnum).length ==1 ){
          hnum = "0" + hnum;
      }
      minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
      mnum = Math.floor(minutes);
      if(String(mnum).length ==1 ){
                mnum = "0" + mnum;
      }
      seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
      snum = Math.round(seconds);
      if(String(snum).length ==1 ){
                snum = "0" + snum;
      }
      document.getElementById("timeDate").innerHTML = "🚀 for&nbsp"+dnum+"&nbspdays";  //此次自定义显示内容
      document.getElementById("times").innerHTML = hnum + "&nbsphr&nbsp" + mnum + "&nbspmin&nbsp" + snum + "&nbspsec";
  }  //此次自定义显示内容
  setInterval("createtime()",250);
  </script>
</div>

  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  




  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  








  

  

  

  

  

  




  
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/caidai.js"></script>



<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body>
</html>
